{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from models import *\n",
    "from ppo import PPO, PPO_RND\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from ppo import PPO\n",
>>>>>>> 28e87e7e8b00547797b0dd0409e45f3b4e11af12
    "from monitor import Monitor\n",
    "from buffer import RolloutStorage\n",
    "from env import *\n",
    "\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "\n",
<<<<<<< HEAD
    "import gym\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybulletgym\n",
    "env = make_vec_env(\"CartPole-v0\", 1)\n",
    "model = PPO(env = env, lr = 0.0003, nstep = 256, batch_size = 256)\n",
    "model.learn(total_timesteps = 1e+7, log_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedDoublePendulumPyBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.shape[0]"
=======
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
>>>>>>> 28e87e7e8b00547797b0dd0409e45f3b4e11af12
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "mlp = MlpContinuous(env.observation_space.shape[0], env.action_space.shape[0])"
=======
    "env = make_env(\"BreakoutNoFrameskip-v4\", 10)"
>>>>>>> 28e87e7e8b00547797b0dd0409e45f3b4e11af12
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "obs = torch.Tensor(env.reset())\n",
    "mlp.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| Progress                | 0.02%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 12       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 2560     |\n",
      "|    total_time           | 3.13     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.09    |\n",
      "|    intrinsic_loss       | 170      |\n",
      "|    policy_gradient_loss | 0.000558 |\n",
      "|    total_loss           | 84.9     |\n",
      "|    value_loss           | 170      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.05%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 24       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 5120     |\n",
      "|    total_time           | 6.42     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.951   |\n",
      "|    intrinsic_loss       | 99.6     |\n",
      "|    policy_gradient_loss | 4.89e-06 |\n",
      "|    total_loss           | 52.8     |\n",
      "|    value_loss           | 99.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.07%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 36       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 7680     |\n",
      "|    total_time           | 9.28     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.876   |\n",
      "|    intrinsic_loss       | 131      |\n",
      "|    policy_gradient_loss | -0.0041  |\n",
      "|    total_loss           | 69       |\n",
      "|    value_loss           | 131      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.1%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 48       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 10240    |\n",
      "|    total_time           | 12.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.824   |\n",
      "|    intrinsic_loss       | 150      |\n",
      "|    policy_gradient_loss | -0.00429 |\n",
      "|    total_loss           | 76.2     |\n",
      "|    value_loss           | 150      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.13%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 64       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 12800    |\n",
      "|    total_time           | 15.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.932   |\n",
      "|    intrinsic_loss       | 143      |\n",
      "|    policy_gradient_loss | -0.00199 |\n",
      "|    total_loss           | 73.5     |\n",
      "|    value_loss           | 143      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.15%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 76        |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 15360     |\n",
      "|    total_time           | 17.8      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.945    |\n",
      "|    intrinsic_loss       | 137       |\n",
      "|    policy_gradient_loss | -0.000391 |\n",
      "|    total_loss           | 71.4      |\n",
      "|    value_loss           | 137       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.18%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 88       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 17920    |\n",
      "|    total_time           | 20.5     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.865   |\n",
      "|    intrinsic_loss       | 73.8     |\n",
      "|    policy_gradient_loss | -0.00282 |\n",
      "|    total_loss           | 63.8     |\n",
      "|    value_loss           | 73.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.2%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 100      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 20480    |\n",
      "|    total_time           | 23.8     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.939   |\n",
      "|    intrinsic_loss       | 118      |\n",
      "|    policy_gradient_loss | -0.00231 |\n",
      "|    total_loss           | 76.7     |\n",
      "|    value_loss           | 118      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.23%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 112       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 23040     |\n",
      "|    total_time           | 27.1      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.93     |\n",
      "|    intrinsic_loss       | 129       |\n",
      "|    policy_gradient_loss | -0.000625 |\n",
      "|    total_loss           | 68.8      |\n",
      "|    value_loss           | 129       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.25%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 128      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 25600    |\n",
      "|    total_time           | 30.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.833   |\n",
      "|    intrinsic_loss       | 126      |\n",
      "|    policy_gradient_loss | -0.00183 |\n",
      "|    total_loss           | 67.9     |\n",
      "|    value_loss           | 126      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.28%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 140       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 28160     |\n",
      "|    total_time           | 33.4      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.964    |\n",
      "|    intrinsic_loss       | 123       |\n",
      "|    policy_gradient_loss | -0.000328 |\n",
      "|    total_loss           | 67        |\n",
      "|    value_loss           | 123       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.3%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 152      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 30720    |\n",
      "|    total_time           | 36.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.867   |\n",
      "|    intrinsic_loss       | 68.3     |\n",
      "|    policy_gradient_loss | -0.00137 |\n",
      "|    total_loss           | 85.3     |\n",
      "|    value_loss           | 68.3     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.33%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 164      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 33280    |\n",
      "|    total_time           | 40.5     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.985   |\n",
      "|    intrinsic_loss       | 117      |\n",
      "|    policy_gradient_loss | 0.00493  |\n",
      "|    total_loss           | 89.3     |\n",
      "|    value_loss           | 117      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.36%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 176      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 35840    |\n",
      "|    total_time           | 43.1     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.916   |\n",
      "|    intrinsic_loss       | 117      |\n",
      "|    policy_gradient_loss | -0.00366 |\n",
      "|    total_loss           | 65.3     |\n",
      "|    value_loss           | 117      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.38%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 192      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 38400    |\n",
      "|    total_time           | 46.9     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.95    |\n",
      "|    intrinsic_loss       | 115      |\n",
      "|    policy_gradient_loss | -0.00152 |\n",
      "|    total_loss           | 64.9     |\n",
      "|    value_loss           | 115      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.41%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 204      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 40960    |\n",
      "|    total_time           | 49.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.991   |\n",
      "|    intrinsic_loss       | 112      |\n",
      "|    policy_gradient_loss | -0.0064  |\n",
      "|    total_loss           | 64       |\n",
      "|    value_loss           | 112      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.43%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 216      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 43520    |\n",
      "|    total_time           | 52.6     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.841   |\n",
      "|    intrinsic_loss       | 69.7     |\n",
      "|    policy_gradient_loss | -0.00461 |\n",
      "|    total_loss           | 113      |\n",
      "|    value_loss           | 69.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.46%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 228      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 46080    |\n",
      "|    total_time           | 56.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.896   |\n",
      "|    intrinsic_loss       | 121      |\n",
      "|    policy_gradient_loss | -0.00858 |\n",
      "|    total_loss           | 106      |\n",
      "|    value_loss           | 121      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.48%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 240      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 48640    |\n",
      "|    total_time           | 59.1     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.836   |\n",
      "|    intrinsic_loss       | 105      |\n",
      "|    policy_gradient_loss | -0.00182 |\n",
      "|    total_loss           | 62.7     |\n",
      "|    value_loss           | 105      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.51%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 256      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 51200    |\n",
      "|    total_time           | 62.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.964   |\n",
      "|    intrinsic_loss       | 103      |\n",
      "|    policy_gradient_loss | -0.00245 |\n",
      "|    total_loss           | 62.3     |\n",
      "|    value_loss           | 103      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.54%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 268      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 53760    |\n",
      "|    total_time           | 66.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.805   |\n",
      "|    intrinsic_loss       | 101      |\n",
      "|    policy_gradient_loss | -0.00309 |\n",
      "|    total_loss           | 61.8     |\n",
      "|    value_loss           | 101      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.56%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 280      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 56320    |\n",
      "|    total_time           | 69.3     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.445   |\n",
      "|    intrinsic_loss       | 76.5     |\n",
      "|    policy_gradient_loss | 0.000488 |\n",
      "|    total_loss           | 148      |\n",
      "|    value_loss           | 76.5     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.59%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 292       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 58880     |\n",
      "|    total_time           | 72.8      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.14     |\n",
      "|    intrinsic_loss       | 125       |\n",
      "|    policy_gradient_loss | -6.81e-05 |\n",
      "|    total_loss           | 125       |\n",
      "|    value_loss           | 125       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.61%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 304       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 61440     |\n",
      "|    total_time           | 75.9      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.284    |\n",
      "|    intrinsic_loss       | 96.2      |\n",
      "|    policy_gradient_loss | -0.000749 |\n",
      "|    total_loss           | 61.2      |\n",
      "|    value_loss           | 96.2      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.64%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 320       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 64000     |\n",
      "|    total_time           | 78.5      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.312    |\n",
      "|    intrinsic_loss       | 93.9      |\n",
      "|    policy_gradient_loss | -0.000447 |\n",
      "|    total_loss           | 60.9      |\n",
      "|    value_loss           | 93.9      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.66%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 332       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 66560     |\n",
      "|    total_time           | 82        |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.344    |\n",
      "|    intrinsic_loss       | 91.8      |\n",
      "|    policy_gradient_loss | -0.000518 |\n",
      "|    total_loss           | 60.8      |\n",
      "|    value_loss           | 91.8      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.69%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 344      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 69120    |\n",
      "|    total_time           | 85.6     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.236   |\n",
      "|    intrinsic_loss       | 86.9     |\n",
      "|    policy_gradient_loss | 0.000808 |\n",
      "|    total_loss           | 185      |\n",
      "|    value_loss           | 86.9     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.71%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 356       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 71680     |\n",
      "|    total_time           | 88.6      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.297    |\n",
      "|    intrinsic_loss       | 130       |\n",
      "|    policy_gradient_loss | -0.000184 |\n",
      "|    total_loss           | 143       |\n",
      "|    value_loss           | 130       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.74%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 368       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 74240     |\n",
      "|    total_time           | 92.2      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.192    |\n",
      "|    intrinsic_loss       | 86.6      |\n",
      "|    policy_gradient_loss | -4.45e-05 |\n",
      "|    total_loss           | 60.2      |\n",
      "|    value_loss           | 86.6      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.77%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 384       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 76800     |\n",
      "|    total_time           | 95.4      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.103    |\n",
      "|    intrinsic_loss       | 85.5      |\n",
      "|    policy_gradient_loss | -7.45e-05 |\n",
      "|    total_loss           | 60.6      |\n",
      "|    value_loss           | 85.5      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.79%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 396       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 79360     |\n",
      "|    total_time           | 98.7      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    intrinsic_loss       | 83.5      |\n",
      "|    policy_gradient_loss | -0.000497 |\n",
      "|    total_loss           | 60.1      |\n",
      "|    value_loss           | 83.5      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.82%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 408      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 81920    |\n",
      "|    total_time           | 102      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.138   |\n",
      "|    intrinsic_loss       | 101      |\n",
      "|    policy_gradient_loss | -0.00101 |\n",
      "|    total_loss           | 228      |\n",
      "|    value_loss           | 101      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.84%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 420      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 84480    |\n",
      "|    total_time           | 105      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0733  |\n",
      "|    intrinsic_loss       | 137      |\n",
      "|    policy_gradient_loss | 8.48e-05 |\n",
      "|    total_loss           | 165      |\n",
      "|    value_loss           | 137      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.87%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 432       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 87040     |\n",
      "|    total_time           | 108       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.1      |\n",
      "|    intrinsic_loss       | 80.3      |\n",
      "|    policy_gradient_loss | -0.000945 |\n",
      "|    total_loss           | 59.9      |\n",
      "|    value_loss           | 80.3      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.89%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 448      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 89600    |\n",
      "|    total_time           | 112      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0515  |\n",
      "|    intrinsic_loss       | 80       |\n",
      "|    policy_gradient_loss | -0.00121 |\n",
      "|    total_loss           | 59.9     |\n",
      "|    value_loss           | 80       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.92%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 460      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 92160    |\n",
      "|    total_time           | 115      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0294  |\n",
      "|    intrinsic_loss       | 79.9     |\n",
      "|    policy_gradient_loss | -0.00119 |\n",
      "|    total_loss           | 59.8     |\n",
      "|    value_loss           | 79.9     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.94%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 472      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 94720    |\n",
      "|    total_time           | 119      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0153  |\n",
      "|    intrinsic_loss       | 107      |\n",
      "|    policy_gradient_loss | 1.09e-09 |\n",
      "|    total_loss           | 245      |\n",
      "|    value_loss           | 107      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.97%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 484      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 97280    |\n",
      "|    total_time           | 122      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0137  |\n",
      "|    intrinsic_loss       | 139      |\n",
      "|    policy_gradient_loss | 0.00029  |\n",
      "|    total_loss           | 171      |\n",
      "|    value_loss           | 139      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 1.0%      |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 496       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 99840     |\n",
      "|    total_time           | 125       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.0134   |\n",
      "|    intrinsic_loss       | 80.8      |\n",
      "|    policy_gradient_loss | -3.57e-09 |\n",
      "|    total_loss           | 60.2      |\n",
      "|    value_loss           | 80.8      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.02%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 512      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 102400   |\n",
      "|    total_time           | 127      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.0135  |\n",
      "|    intrinsic_loss       | 80.1     |\n",
      "|    policy_gradient_loss | 6.03e-09 |\n",
      "|    total_loss           | 60       |\n",
      "|    value_loss           | 80.1     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 1.05%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 524       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 104960    |\n",
      "|    total_time           | 130       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.016    |\n",
      "|    intrinsic_loss       | 80        |\n",
      "|    policy_gradient_loss | -1.05e-08 |\n",
      "|    total_loss           | 59.8      |\n",
      "|    value_loss           | 80        |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ab36fe0b3884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MountainCar-v0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec_env_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO_RND\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.03\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e+7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, log_interval, n_eval_episodes)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mcollect_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                 \u001b[0mint_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mext_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\models.py\u001b[0m in \u001b[0;36mint_reward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mint_rew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\models.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mint_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1608\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1609\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pybulletgym\n",
    "env = make_vec_env(\"MountainCar-v0\", 4, vec_env_cls = SubprocVecEnv)\n",
    "model = PPO_RND(env = env, lr = 0.03, nstep = 64, batch_size = 64, max_grad_norm = 0.1, hidden_size = 16)\n",
    "model.learn(total_timesteps = 1e+7, log_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "\n",
    "algorithm = PPO(env = env)\n",
    "algorithm.learn(1e+06, log_interval = 10)"
   ]
>>>>>>> 28e87e7e8b00547797b0dd0409e45f3b4e11af12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
