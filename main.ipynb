{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from models import *\n",
    "from ppo import PPO, PPO_RND, PPO_ICM\n",
    "from buffer import RolloutStorage\n",
    "from env import *\n",
    "\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import vec_normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybulletgym\n",
    "env = make_vec_env(\"CartPole-v0\", 1)\n",
    "env = VecNormalize(env)\n",
    "model = PPO(env = env, lr = 0.0003, nstep = 256, batch_size = 256)\n",
    "model.learn(total_timesteps = 1e+7, log_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybulletgym\n",
    "env = make_vec_env(\"MountainCar-v0\", 4, vec_env_cls = SubprocVecEnv)\n",
    "env = VecNormalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| Progress                | 0.05%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 200       |\n",
      "|    ep_rew_mean          | -200      |\n",
      "|    num_episodes         | 24        |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 5120      |\n",
      "|    total_time           | 4.36      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -1.09     |\n",
      "|    icm_loss             | 0.745     |\n",
      "|    policy_gradient_loss | -3.91e-05 |\n",
      "|    total_loss           | 64.8      |\n",
      "|    value_loss           | 64.1      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.1%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 200      |\n",
      "|    ep_rew_mean          | -200     |\n",
      "|    num_episodes         | 48       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 10240    |\n",
      "|    total_time           | 8.9      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.07    |\n",
      "|    icm_loss             | 0.741    |\n",
      "|    policy_gradient_loss | -0.0014  |\n",
      "|    total_loss           | 62.7     |\n",
      "|    value_loss           | 61.9     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.15%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 195      |\n",
      "|    ep_rew_mean          | -195     |\n",
      "|    num_episodes         | 77       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 15360    |\n",
      "|    total_time           | 13.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.552   |\n",
      "|    icm_loss             | 0.542    |\n",
      "|    policy_gradient_loss | -0.00231 |\n",
      "|    total_loss           | 107      |\n",
      "|    value_loss           | 106      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.2%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 180      |\n",
      "|    ep_rew_mean          | -180     |\n",
      "|    num_episodes         | 111      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 20480    |\n",
      "|    total_time           | 17.8     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.351   |\n",
      "|    icm_loss             | 0.38     |\n",
      "|    policy_gradient_loss | 0.00128  |\n",
      "|    total_loss           | 107      |\n",
      "|    value_loss           | 106      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.25%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 162       |\n",
      "|    ep_rew_mean          | -162      |\n",
      "|    num_episodes         | 146       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 25600     |\n",
      "|    total_time           | 22.3      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.251    |\n",
      "|    icm_loss             | 0.283     |\n",
      "|    policy_gradient_loss | -0.000183 |\n",
      "|    total_loss           | 92.4      |\n",
      "|    value_loss           | 92.1      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.3%      |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 148       |\n",
      "|    ep_rew_mean          | -148      |\n",
      "|    num_episodes         | 181       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 30720     |\n",
      "|    total_time           | 26.7      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.272    |\n",
      "|    icm_loss             | 0.312     |\n",
      "|    policy_gradient_loss | -0.000311 |\n",
      "|    total_loss           | 57.2      |\n",
      "|    value_loss           | 56.9      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.35%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 145      |\n",
      "|    ep_rew_mean          | -145     |\n",
      "|    num_episodes         | 216      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 35840    |\n",
      "|    total_time           | 30.9     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.266   |\n",
      "|    icm_loss             | 0.287    |\n",
      "|    policy_gradient_loss | 0.000558 |\n",
      "|    total_loss           | 23.6     |\n",
      "|    value_loss           | 23.3     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.4%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 140      |\n",
      "|    ep_rew_mean          | -140     |\n",
      "|    num_episodes         | 255      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 40960    |\n",
      "|    total_time           | 35.8     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.368   |\n",
      "|    icm_loss             | 0.422    |\n",
      "|    policy_gradient_loss | 0.0055   |\n",
      "|    total_loss           | 82.3     |\n",
      "|    value_loss           | 81.9     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.46%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 142      |\n",
      "|    ep_rew_mean          | -142     |\n",
      "|    num_episodes         | 289      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 46080    |\n",
      "|    total_time           | 40.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.317   |\n",
      "|    icm_loss             | 0.362    |\n",
      "|    policy_gradient_loss | 0.00121  |\n",
      "|    total_loss           | 31.5     |\n",
      "|    value_loss           | 31.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.51%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 142      |\n",
      "|    ep_rew_mean          | -142     |\n",
      "|    num_episodes         | 325      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 51200    |\n",
      "|    total_time           | 44.6     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.223   |\n",
      "|    icm_loss             | 0.297    |\n",
      "|    policy_gradient_loss | -0.00185 |\n",
      "|    total_loss           | 18.2     |\n",
      "|    value_loss           | 17.9     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.56%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 144      |\n",
      "|    ep_rew_mean          | -144     |\n",
      "|    num_episodes         | 361      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 56320    |\n",
      "|    total_time           | 49       |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.219   |\n",
      "|    icm_loss             | 0.241    |\n",
      "|    policy_gradient_loss | -0.00118 |\n",
      "|    total_loss           | 24       |\n",
      "|    value_loss           | 23.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.61%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 146      |\n",
      "|    ep_rew_mean          | -146     |\n",
      "|    num_episodes         | 394      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 61440    |\n",
      "|    total_time           | 53.6     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.162   |\n",
      "|    icm_loss             | 0.232    |\n",
      "|    policy_gradient_loss | -0.00377 |\n",
      "|    total_loss           | 21.4     |\n",
      "|    value_loss           | 21.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.66%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 158      |\n",
      "|    ep_rew_mean          | -158     |\n",
      "|    num_episodes         | 422      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 66560    |\n",
      "|    total_time           | 58.1     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.131   |\n",
      "|    icm_loss             | 0.234    |\n",
      "|    policy_gradient_loss | 0.00391  |\n",
      "|    total_loss           | 41.5     |\n",
      "|    value_loss           | 41.3     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-877efe02a684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO_ICM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0micm_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e+7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, log_interval)\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mcollect_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_returns_and_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\buffer.py\u001b[0m in \u001b[0;36mcompute_returns_and_advantages\u001b[1;34m(self, last_value, dones)\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlast_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m                 \u001b[0mnext_non_terminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m                 \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_value\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_non_terminal\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO_ICM(env = env, lr = 0.003, nstep = 128, batch_size = 128, max_grad_norm = 0.5, hidden_size = 16, icm_hidden_size = 128)\n",
    "model.learn(total_timesteps = 1e+7, log_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
