{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from models import *\n",
    "from ppo import PPO, PPO_RND\n",
    "from monitor import Monitor\n",
    "from buffer import RolloutStorage\n",
    "from env import *\n",
    "\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.common.cmd_util import make_vec_env\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| Progress                | 0.05%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 105      |\n",
      "|    ep_rew_mean          | -182     |\n",
      "|    num_episodes         | 47       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 5120     |\n",
      "|    total_time           | 3.07     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.3     |\n",
      "|    policy_gradient_loss | -0.0145  |\n",
      "|    total_loss           | 455      |\n",
      "|    value_loss           | 911      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.1%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 114      |\n",
      "|    ep_rew_mean          | -127     |\n",
      "|    num_episodes         | 89       |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 10240    |\n",
      "|    total_time           | 6.76     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.22    |\n",
      "|    policy_gradient_loss | -0.00809 |\n",
      "|    total_loss           | 77.3     |\n",
      "|    value_loss           | 155      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.15%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 126      |\n",
      "|    ep_rew_mean          | -84.7    |\n",
      "|    num_episodes         | 120      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 15360    |\n",
      "|    total_time           | 11.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.17    |\n",
      "|    policy_gradient_loss | -0.00378 |\n",
      "|    total_loss           | 66.1     |\n",
      "|    value_loss           | 132      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.2%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 158      |\n",
      "|    ep_rew_mean          | -70.8    |\n",
      "|    num_episodes         | 128      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 20480    |\n",
      "|    total_time           | 16.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.11    |\n",
      "|    policy_gradient_loss | -0.00183 |\n",
      "|    total_loss           | 20.4     |\n",
      "|    value_loss           | 40.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.25%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 198      |\n",
      "|    ep_rew_mean          | -40.2    |\n",
      "|    num_episodes         | 150      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 25600    |\n",
      "|    total_time           | 20.4     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -1.02    |\n",
      "|    policy_gradient_loss | -0.00782 |\n",
      "|    total_loss           | 14.8     |\n",
      "|    value_loss           | 29.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.3%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 223      |\n",
      "|    ep_rew_mean          | -18.3    |\n",
      "|    num_episodes         | 171      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 30720    |\n",
      "|    total_time           | 24.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.841   |\n",
      "|    policy_gradient_loss | -0.00524 |\n",
      "|    total_loss           | 23.2     |\n",
      "|    value_loss           | 46.3     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.35%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 232      |\n",
      "|    ep_rew_mean          | 3.91     |\n",
      "|    num_episodes         | 193      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 35840    |\n",
      "|    total_time           | 28.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.696   |\n",
      "|    policy_gradient_loss | 0.000627 |\n",
      "|    total_loss           | 21.3     |\n",
      "|    value_loss           | 42.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.4%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 271      |\n",
      "|    ep_rew_mean          | 35.3     |\n",
      "|    num_episodes         | 218      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 40960    |\n",
      "|    total_time           | 32.5     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.58    |\n",
      "|    policy_gradient_loss | 0.00113  |\n",
      "|    total_loss           | 58.3     |\n",
      "|    value_loss           | 117      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.46%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 236      |\n",
      "|    ep_rew_mean          | 65.4     |\n",
      "|    num_episodes         | 238      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 46080    |\n",
      "|    total_time           | 36.3     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.788   |\n",
      "|    policy_gradient_loss | -0.00115 |\n",
      "|    total_loss           | 16.8     |\n",
      "|    value_loss           | 33.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.51%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 222      |\n",
      "|    ep_rew_mean          | 87.1     |\n",
      "|    num_episodes         | 266      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 51200    |\n",
      "|    total_time           | 40.4     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.723   |\n",
      "|    policy_gradient_loss | -0.00302 |\n",
      "|    total_loss           | 13.8     |\n",
      "|    value_loss           | 27.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.56%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 223      |\n",
      "|    ep_rew_mean          | 99.5     |\n",
      "|    num_episodes         | 291      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 56320    |\n",
      "|    total_time           | 44.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.629   |\n",
      "|    policy_gradient_loss | -0.00526 |\n",
      "|    total_loss           | 43.6     |\n",
      "|    value_loss           | 87.3     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.61%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 207      |\n",
      "|    ep_rew_mean          | 111      |\n",
      "|    num_episodes         | 313      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 61440    |\n",
      "|    total_time           | 47.9     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.62    |\n",
      "|    policy_gradient_loss | -0.00102 |\n",
      "|    total_loss           | 39.7     |\n",
      "|    value_loss           | 79.4     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.66%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 216      |\n",
      "|    ep_rew_mean          | 106      |\n",
      "|    num_episodes         | 333      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 66560    |\n",
      "|    total_time           | 51.7     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.614   |\n",
      "|    policy_gradient_loss | 0.00172  |\n",
      "|    total_loss           | 71.2     |\n",
      "|    value_loss           | 142      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.71%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 231      |\n",
      "|    ep_rew_mean          | 106      |\n",
      "|    num_episodes         | 341      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 71680    |\n",
      "|    total_time           | 56.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.856   |\n",
      "|    policy_gradient_loss | -0.0103  |\n",
      "|    total_loss           | 15.6     |\n",
      "|    value_loss           | 31.3     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 0.76%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 272       |\n",
      "|    ep_rew_mean          | 95.1      |\n",
      "|    num_episodes         | 349       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 76800     |\n",
      "|    total_time           | 60.9      |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.826    |\n",
      "|    policy_gradient_loss | -0.000101 |\n",
      "|    total_loss           | 31.5      |\n",
      "|    value_loss           | 63        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.81%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 306      |\n",
      "|    ep_rew_mean          | 95.1     |\n",
      "|    num_episodes         | 355      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 81920    |\n",
      "|    total_time           | 65.4     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.655   |\n",
      "|    policy_gradient_loss | -0.00434 |\n",
      "|    total_loss           | 3.63     |\n",
      "|    value_loss           | 7.28     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.87%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 347      |\n",
      "|    ep_rew_mean          | 105      |\n",
      "|    num_episodes         | 366      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 87040    |\n",
      "|    total_time           | 69.9     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.611   |\n",
      "|    policy_gradient_loss | -0.0151  |\n",
      "|    total_loss           | 39.1     |\n",
      "|    value_loss           | 78.3     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.92%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 381      |\n",
      "|    ep_rew_mean          | 117      |\n",
      "|    num_episodes         | 382      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 92160    |\n",
      "|    total_time           | 74.1     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.572   |\n",
      "|    policy_gradient_loss | 0.0047   |\n",
      "|    total_loss           | 41       |\n",
      "|    value_loss           | 82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 0.97%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 398      |\n",
      "|    ep_rew_mean          | 122      |\n",
      "|    num_episodes         | 395      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 97280    |\n",
      "|    total_time           | 78.4     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.675   |\n",
      "|    policy_gradient_loss | 0.0109   |\n",
      "|    total_loss           | 42.9     |\n",
      "|    value_loss           | 85.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.02%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 424      |\n",
      "|    ep_rew_mean          | 123      |\n",
      "|    num_episodes         | 407      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 102400   |\n",
      "|    total_time           | 82.6     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.774   |\n",
      "|    policy_gradient_loss | 0.004    |\n",
      "|    total_loss           | 9.42     |\n",
      "|    value_loss           | 18.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.07%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 416      |\n",
      "|    ep_rew_mean          | 109      |\n",
      "|    num_episodes         | 432      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 107520   |\n",
      "|    total_time           | 87       |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.677   |\n",
      "|    policy_gradient_loss | -0.00214 |\n",
      "|    total_loss           | 9.8      |\n",
      "|    value_loss           | 19.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.12%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 323      |\n",
      "|    ep_rew_mean          | 100      |\n",
      "|    num_episodes         | 455      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 112640   |\n",
      "|    total_time           | 91       |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.784   |\n",
      "|    policy_gradient_loss | -0.0102  |\n",
      "|    total_loss           | 5.8      |\n",
      "|    value_loss           | 11.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.17%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 275      |\n",
      "|    ep_rew_mean          | 97       |\n",
      "|    num_episodes         | 474      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 117760   |\n",
      "|    total_time           | 95       |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.594   |\n",
      "|    policy_gradient_loss | -0.00359 |\n",
      "|    total_loss           | 5.36     |\n",
      "|    value_loss           | 10.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.22%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 292      |\n",
      "|    ep_rew_mean          | 103      |\n",
      "|    num_episodes         | 482      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 122880   |\n",
      "|    total_time           | 99.2     |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.779   |\n",
      "|    policy_gradient_loss | -0.00155 |\n",
      "|    total_loss           | 20.9     |\n",
      "|    value_loss           | 41.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.27%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 320      |\n",
      "|    ep_rew_mean          | 98.5     |\n",
      "|    num_episodes         | 492      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 128000   |\n",
      "|    total_time           | 104      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.622   |\n",
      "|    policy_gradient_loss | -0.00321 |\n",
      "|    total_loss           | 43.6     |\n",
      "|    value_loss           | 87.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.33%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 315      |\n",
      "|    ep_rew_mean          | 110      |\n",
      "|    num_episodes         | 507      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 133120   |\n",
      "|    total_time           | 108      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.55    |\n",
      "|    policy_gradient_loss | -0.00131 |\n",
      "|    total_loss           | 27.6     |\n",
      "|    value_loss           | 55.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.38%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 320      |\n",
      "|    ep_rew_mean          | 136      |\n",
      "|    num_episodes         | 525      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 138240   |\n",
      "|    total_time           | 112      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.508   |\n",
      "|    policy_gradient_loss | -0.00224 |\n",
      "|    total_loss           | 47.4     |\n",
      "|    value_loss           | 94.8     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 1.43%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 334       |\n",
      "|    ep_rew_mean          | 151       |\n",
      "|    num_episodes         | 534       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 143360    |\n",
      "|    total_time           | 116       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.67     |\n",
      "|    policy_gradient_loss | -0.000605 |\n",
      "|    total_loss           | 3.31      |\n",
      "|    value_loss           | 6.64      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.48%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 376      |\n",
      "|    ep_rew_mean          | 153      |\n",
      "|    num_episodes         | 547      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 148480   |\n",
      "|    total_time           | 121      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.628   |\n",
      "|    policy_gradient_loss | 0.00519  |\n",
      "|    total_loss           | 32.4     |\n",
      "|    value_loss           | 64.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.53%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 385      |\n",
      "|    ep_rew_mean          | 155      |\n",
      "|    num_episodes         | 564      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 153600   |\n",
      "|    total_time           | 125      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.555   |\n",
      "|    policy_gradient_loss | 0.000788 |\n",
      "|    total_loss           | 23.6     |\n",
      "|    value_loss           | 47.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.58%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 373      |\n",
      "|    ep_rew_mean          | 142      |\n",
      "|    num_episodes         | 581      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 158720   |\n",
      "|    total_time           | 129      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.588   |\n",
      "|    policy_gradient_loss | 0.00593  |\n",
      "|    total_loss           | 58.1     |\n",
      "|    value_loss           | 116      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.63%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 349      |\n",
      "|    ep_rew_mean          | 122      |\n",
      "|    num_episodes         | 595      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 163840   |\n",
      "|    total_time           | 133      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.746   |\n",
      "|    policy_gradient_loss | -0.00588 |\n",
      "|    total_loss           | 75.1     |\n",
      "|    value_loss           | 150      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.68%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 343      |\n",
      "|    ep_rew_mean          | 128      |\n",
      "|    num_episodes         | 610      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 168960   |\n",
      "|    total_time           | 137      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.565   |\n",
      "|    policy_gradient_loss | -0.00162 |\n",
      "|    total_loss           | 31.9     |\n",
      "|    value_loss           | 63.9     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.74%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 371      |\n",
      "|    ep_rew_mean          | 132      |\n",
      "|    num_episodes         | 623      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 174080   |\n",
      "|    total_time           | 142      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.515   |\n",
      "|    policy_gradient_loss | -0.00847 |\n",
      "|    total_loss           | 30.4     |\n",
      "|    value_loss           | 60.8     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.79%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 386      |\n",
      "|    ep_rew_mean          | 123      |\n",
      "|    num_episodes         | 631      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 179200   |\n",
      "|    total_time           | 147      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.687   |\n",
      "|    policy_gradient_loss | 1.1e-05  |\n",
      "|    total_loss           | 24       |\n",
      "|    value_loss           | 48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.84%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 376      |\n",
      "|    ep_rew_mean          | 129      |\n",
      "|    num_episodes         | 641      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 184320   |\n",
      "|    total_time           | 152      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.666   |\n",
      "|    policy_gradient_loss | 0.0102   |\n",
      "|    total_loss           | 54.6     |\n",
      "|    value_loss           | 109      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.89%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 385      |\n",
      "|    ep_rew_mean          | 130      |\n",
      "|    num_episodes         | 650      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 189440   |\n",
      "|    total_time           | 157      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.682   |\n",
      "|    policy_gradient_loss | -0.00302 |\n",
      "|    total_loss           | 0.596    |\n",
      "|    value_loss           | 1.21     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.94%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 431      |\n",
      "|    ep_rew_mean          | 128      |\n",
      "|    num_episodes         | 658      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 194560   |\n",
      "|    total_time           | 161      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.758   |\n",
      "|    policy_gradient_loss | -0.00804 |\n",
      "|    total_loss           | 7.22     |\n",
      "|    value_loss           | 14.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 1.99%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 428      |\n",
      "|    ep_rew_mean          | 111      |\n",
      "|    num_episodes         | 675      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 199680   |\n",
      "|    total_time           | 166      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.727   |\n",
      "|    policy_gradient_loss | 0.000664 |\n",
      "|    total_loss           | 65.7     |\n",
      "|    value_loss           | 131      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.04%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 437      |\n",
      "|    ep_rew_mean          | 102      |\n",
      "|    num_episodes         | 689      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 204800   |\n",
      "|    total_time           | 170      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.925   |\n",
      "|    policy_gradient_loss | 0.000274 |\n",
      "|    total_loss           | 15.7     |\n",
      "|    value_loss           | 31.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.09%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 442      |\n",
      "|    ep_rew_mean          | 85.6     |\n",
      "|    num_episodes         | 698      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 209920   |\n",
      "|    total_time           | 175      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.888   |\n",
      "|    policy_gradient_loss | -0.00483 |\n",
      "|    total_loss           | 1.65     |\n",
      "|    value_loss           | 3.33     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.15%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 465      |\n",
      "|    ep_rew_mean          | 52.4     |\n",
      "|    num_episodes         | 710      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 215040   |\n",
      "|    total_time           | 180      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.753   |\n",
      "|    policy_gradient_loss | -0.00195 |\n",
      "|    total_loss           | 29.1     |\n",
      "|    value_loss           | 58.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.2%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 459      |\n",
      "|    ep_rew_mean          | 34.1     |\n",
      "|    num_episodes         | 722      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 220160   |\n",
      "|    total_time           | 185      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.836   |\n",
      "|    policy_gradient_loss | 0.00431  |\n",
      "|    total_loss           | 23.6     |\n",
      "|    value_loss           | 47.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.25%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 444      |\n",
      "|    ep_rew_mean          | 25.4     |\n",
      "|    num_episodes         | 735      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 225280   |\n",
      "|    total_time           | 190      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.757   |\n",
      "|    policy_gradient_loss | 0.00724  |\n",
      "|    total_loss           | 36.6     |\n",
      "|    value_loss           | 73.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.3%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 435      |\n",
      "|    ep_rew_mean          | 17.6     |\n",
      "|    num_episodes         | 744      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 230400   |\n",
      "|    total_time           | 195      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.73    |\n",
      "|    policy_gradient_loss | -0.00421 |\n",
      "|    total_loss           | 15.1     |\n",
      "|    value_loss           | 30.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.35%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 421      |\n",
      "|    ep_rew_mean          | 25.7     |\n",
      "|    num_episodes         | 755      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 235520   |\n",
      "|    total_time           | 199      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.904   |\n",
      "|    policy_gradient_loss | -0.00963 |\n",
      "|    total_loss           | 1.16     |\n",
      "|    value_loss           | 2.36     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 2.4%      |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 430       |\n",
      "|    ep_rew_mean          | 38.4      |\n",
      "|    num_episodes         | 770       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 240640    |\n",
      "|    total_time           | 204       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.692    |\n",
      "|    policy_gradient_loss | -0.000761 |\n",
      "|    total_loss           | 21.8      |\n",
      "|    value_loss           | 43.5      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 2.45%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 421       |\n",
      "|    ep_rew_mean          | 70.1      |\n",
      "|    num_episodes         | 785       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 245760    |\n",
      "|    total_time           | 208       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.445    |\n",
      "|    policy_gradient_loss | -0.000572 |\n",
      "|    total_loss           | 53.3      |\n",
      "|    value_loss           | 107       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.5%     |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 367      |\n",
      "|    ep_rew_mean          | 100      |\n",
      "|    num_episodes         | 808      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 250880   |\n",
      "|    total_time           | 212      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.763   |\n",
      "|    policy_gradient_loss | -0.0122  |\n",
      "|    total_loss           | 11       |\n",
      "|    value_loss           | 22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.55%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 357      |\n",
      "|    ep_rew_mean          | 123      |\n",
      "|    num_episodes         | 822      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 256000   |\n",
      "|    total_time           | 217      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.371   |\n",
      "|    policy_gradient_loss | -0.00259 |\n",
      "|    total_loss           | 5.48     |\n",
      "|    value_loss           | 11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.61%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 366      |\n",
      "|    ep_rew_mean          | 131      |\n",
      "|    num_episodes         | 831      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 261120   |\n",
      "|    total_time           | 221      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.788   |\n",
      "|    policy_gradient_loss | -0.00415 |\n",
      "|    total_loss           | 1.13     |\n",
      "|    value_loss           | 2.27     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.66%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 367      |\n",
      "|    ep_rew_mean          | 144      |\n",
      "|    num_episodes         | 845      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 266240   |\n",
      "|    total_time           | 226      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.87    |\n",
      "|    policy_gradient_loss | -0.00405 |\n",
      "|    total_loss           | 5.17     |\n",
      "|    value_loss           | 10.4     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 2.71%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 349       |\n",
      "|    ep_rew_mean          | 160       |\n",
      "|    num_episodes         | 859       |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 271360    |\n",
      "|    total_time           | 230       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.85     |\n",
      "|    policy_gradient_loss | -0.000528 |\n",
      "|    total_loss           | 30.3      |\n",
      "|    value_loss           | 60.5      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.76%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 352      |\n",
      "|    ep_rew_mean          | 158      |\n",
      "|    num_episodes         | 872      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 276480   |\n",
      "|    total_time           | 235      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.661   |\n",
      "|    policy_gradient_loss | -0.00844 |\n",
      "|    total_loss           | 4.04     |\n",
      "|    value_loss           | 8.1      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.81%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 333      |\n",
      "|    ep_rew_mean          | 151      |\n",
      "|    num_episodes         | 893      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 281600   |\n",
      "|    total_time           | 239      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.838   |\n",
      "|    policy_gradient_loss | -0.00123 |\n",
      "|    total_loss           | 30.6     |\n",
      "|    value_loss           | 61.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.86%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 352      |\n",
      "|    ep_rew_mean          | 159      |\n",
      "|    num_episodes         | 911      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 286720   |\n",
      "|    total_time           | 243      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.868   |\n",
      "|    policy_gradient_loss | -0.00441 |\n",
      "|    total_loss           | 11.6     |\n",
      "|    value_loss           | 23.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.91%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 315      |\n",
      "|    ep_rew_mean          | 144      |\n",
      "|    num_episodes         | 930      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 291840   |\n",
      "|    total_time           | 248      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.737   |\n",
      "|    policy_gradient_loss | -0.00985 |\n",
      "|    total_loss           | 26.3     |\n",
      "|    value_loss           | 52.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 2.96%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 320      |\n",
      "|    ep_rew_mean          | 148      |\n",
      "|    num_episodes         | 941      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 296960   |\n",
      "|    total_time           | 252      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.843   |\n",
      "|    policy_gradient_loss | -0.00513 |\n",
      "|    total_loss           | 22.8     |\n",
      "|    value_loss           | 45.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.02%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 318      |\n",
      "|    ep_rew_mean          | 154      |\n",
      "|    num_episodes         | 954      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 302080   |\n",
      "|    total_time           | 257      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.486   |\n",
      "|    policy_gradient_loss | -0.00158 |\n",
      "|    total_loss           | 46.8     |\n",
      "|    value_loss           | 93.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.07%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 302      |\n",
      "|    ep_rew_mean          | 159      |\n",
      "|    num_episodes         | 973      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 307200   |\n",
      "|    total_time           | 261      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.679   |\n",
      "|    policy_gradient_loss | -0.00321 |\n",
      "|    total_loss           | 60.6     |\n",
      "|    value_loss           | 121      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.12%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 321      |\n",
      "|    ep_rew_mean          | 176      |\n",
      "|    num_episodes         | 989      |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 312320   |\n",
      "|    total_time           | 266      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.478   |\n",
      "|    policy_gradient_loss | -0.00113 |\n",
      "|    total_loss           | 46.3     |\n",
      "|    value_loss           | 92.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.17%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 310      |\n",
      "|    ep_rew_mean          | 190      |\n",
      "|    num_episodes         | 1007     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 317440   |\n",
      "|    total_time           | 271      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.718   |\n",
      "|    policy_gradient_loss | -0.00523 |\n",
      "|    total_loss           | 6.91     |\n",
      "|    value_loss           | 13.8     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| Progress                | 3.22%     |\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 324       |\n",
      "|    ep_rew_mean          | 215       |\n",
      "|    num_episodes         | 1024      |\n",
      "| time/                   |           |\n",
      "|    total timesteps      | 322560    |\n",
      "|    total_time           | 275       |\n",
      "| train/                  |           |\n",
      "|    entropy_loss         | -0.701    |\n",
      "|    policy_gradient_loss | -0.000822 |\n",
      "|    total_loss           | 41.2      |\n",
      "|    value_loss           | 82.4      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.27%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 308      |\n",
      "|    ep_rew_mean          | 206      |\n",
      "|    num_episodes         | 1042     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 327680   |\n",
      "|    total_time           | 281      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.806   |\n",
      "|    policy_gradient_loss | -0.0146  |\n",
      "|    total_loss           | 2.19     |\n",
      "|    value_loss           | 4.43     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.32%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 300      |\n",
      "|    ep_rew_mean          | 198      |\n",
      "|    num_episodes         | 1058     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 332800   |\n",
      "|    total_time           | 285      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.346   |\n",
      "|    policy_gradient_loss | 0.017    |\n",
      "|    total_loss           | 0.407    |\n",
      "|    value_loss           | 0.787    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.37%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 310      |\n",
      "|    ep_rew_mean          | 202      |\n",
      "|    num_episodes         | 1073     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 337920   |\n",
      "|    total_time           | 289      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.478   |\n",
      "|    policy_gradient_loss | -0.00103 |\n",
      "|    total_loss           | 319      |\n",
      "|    value_loss           | 639      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.43%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 307      |\n",
      "|    ep_rew_mean          | 202      |\n",
      "|    num_episodes         | 1090     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 343040   |\n",
      "|    total_time           | 293      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.767   |\n",
      "|    policy_gradient_loss | -0.00448 |\n",
      "|    total_loss           | 25.5     |\n",
      "|    value_loss           | 51.1     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.48%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 323      |\n",
      "|    ep_rew_mean          | 206      |\n",
      "|    num_episodes         | 1098     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 348160   |\n",
      "|    total_time           | 298      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.819   |\n",
      "|    policy_gradient_loss | -0.00149 |\n",
      "|    total_loss           | 6.7      |\n",
      "|    value_loss           | 13.4     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.53%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 344      |\n",
      "|    ep_rew_mean          | 173      |\n",
      "|    num_episodes         | 1109     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 353280   |\n",
      "|    total_time           | 303      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.773   |\n",
      "|    policy_gradient_loss | -0.00345 |\n",
      "|    total_loss           | 148      |\n",
      "|    value_loss           | 296      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.58%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 368      |\n",
      "|    ep_rew_mean          | 158      |\n",
      "|    num_episodes         | 1117     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 358400   |\n",
      "|    total_time           | 308      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.956   |\n",
      "|    policy_gradient_loss | -0.0039  |\n",
      "|    total_loss           | 24.7     |\n",
      "|    value_loss           | 49.5     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.63%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 394      |\n",
      "|    ep_rew_mean          | 147      |\n",
      "|    num_episodes         | 1126     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 363520   |\n",
      "|    total_time           | 313      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.798   |\n",
      "|    policy_gradient_loss | 0.000503 |\n",
      "|    total_loss           | 28.3     |\n",
      "|    value_loss           | 56.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.68%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 415      |\n",
      "|    ep_rew_mean          | 151      |\n",
      "|    num_episodes         | 1140     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 368640   |\n",
      "|    total_time           | 317      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.613   |\n",
      "|    policy_gradient_loss | -0.00248 |\n",
      "|    total_loss           | 334      |\n",
      "|    value_loss           | 669      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.73%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 435      |\n",
      "|    ep_rew_mean          | 145      |\n",
      "|    num_episodes         | 1150     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 373760   |\n",
      "|    total_time           | 322      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.77    |\n",
      "|    policy_gradient_loss | -0.00387 |\n",
      "|    total_loss           | 31.3     |\n",
      "|    value_loss           | 62.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| Progress                | 3.78%    |\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 452      |\n",
      "|    ep_rew_mean          | 126      |\n",
      "|    num_episodes         | 1159     |\n",
      "| time/                   |          |\n",
      "|    total timesteps      | 378880   |\n",
      "|    total_time           | 327      |\n",
      "| train/                  |          |\n",
      "|    entropy_loss         | -0.728   |\n",
      "|    policy_gradient_loss | 0.00371  |\n",
      "|    total_loss           | 64.6     |\n",
      "|    value_loss           | 129      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f14b27dad222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LunarLander-v2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec_env_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.003\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e+7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, log_interval)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\ppo.py\u001b[0m in \u001b[0;36mcollect_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_info_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc Data Analytics\\Reinforcement Learning\\PPO\\buffer.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, obs, action, reward, value, mask, log_prob)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m              \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_log_probs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mlog_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pybulletgym\n",
    "env = make_vec_env(\"LunarLander-v2\", 4, vec_env_cls = SubprocVecEnv)\n",
    "model = PPO(env = env, lr = 0.003, nstep = 128)\n",
    "model.learn(total_timesteps = 1e+7, log_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
